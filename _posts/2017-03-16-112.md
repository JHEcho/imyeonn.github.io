---
title: "개인 프로젝트 경과 보고"
layout: post
date: 2017-03-16 08:40
image: /assets/images/post/001/04_00title.png
headerImage: true
category: E-Commerce
tag:
- 개인 프로젝트
- 뷰투
- 포트폴리오
blog: true
author: Hyeyeon
description: 개인 프로젝트 경과 보고
---

### Summary:

개인 프로젝트가 마무리 과정 중에 있어 그 경과를 기록했습니다.

---

유튜브를 좋아한다. 특히 뷰티 유튜브를 많이 본다. 유튜브를 보다 보면 좋은 제품을 많이 건진다. 그 때 그 때 따로 메모해놓으면 좋으련만 당시에는 별 느낌이 없었던 제품들이 나중에 되서야 '아 그거 뭐였지? 뭔가 사고 싶은데 그거 사고 싶다...' 하는 생각이 들 때가 많다. 유튜브 검색창에 유튜버 이름과 제품명을 검색해본다. 안 나온다. 제품명이 이게 아닌가? 이 유튜버가 사용한 게 아니였나? 기억이 나지 않는다. 뭐지, 사고 싶은데.

이런 나의 불편을 해결해보고자 만든 서비스가 **뷰투** 다. 내가 제일 좋아하는 유튜버인 유트루님의 영상들을 기반으로 제품 검색 서비스를 만들었다. 차차 기획, 개발, 디자인 과정을 기록하겠지만 험난했다. 매우 험난했다. 이 포스팅은 서비스가 완성되기 전까지 있었던 주요 이슈들에 대한 것이다.


### 일본어

처음엔 MS 엑셀로 데이터 기반 작업을 했다. 파이썬 크롤러로 영상정보를 끌어와서 csv에 옮기고 제품정보는 하나씩 찾아보며 입력했다. 그리고 꽤 진전이 있었던 다음 날 엑셀을 열어보니 한글이 일본어가 되어 있었다. 하... 윈도와 맥을 번갈아 이용한 적 없고 인코딩은 utf-8이었다. 근데 왜? 도대체 왜? 복구할 수 있는 방법을 찾지 못했다. 다 날렸다. 한글과 엑셀의 조합은 위험하다는 교훈을 얻고 구글드라이브의 구글스프레드시트로 갈아탔다.

![pic8](/assets/images/post/002/112_08.png)

---

<br>

### 데이터베이스 구축

파이썬 크롤러를 배워 영상정보를 크롤링할 계획이었다. 파이썬 스터디에도 나갔다. 열심히 배웠다. 그 결과 구글에 돌아다니는 코드들로 유툽 크롤링 코드를 짜집기할 수 있었다. 그러나 진짜 핵심 정보는 크롤링이 불가능했다. 유튜버가 어떠한 영상에 대해 그 영상 속에 등장한 제품들을 특정 기준에 맞춰 정리해놓지 않았기 때문이다.

영상을 하나씩 돌려보면서 영상 속에 등장하는 제품의 제품명, 제품분류코드, 제품사진, 제품판매링크를 개별적으로 받아적었다. 모든 것이 수작업이었다. 이 때 일주일동안 서비스를 완성하겠다는 건 내 헛된 희망이었다는 것을 예감했다. 모든 데이터를 수집하는데만 꼬박 일주일이 걸렸다.

![pic1](/assets/images/post/002/112_01.png)
<figcaption class="caption">영상 데이터 수집 달성률</figcaption>

---

<br>

### 트래픽 초과

닷홈의 무료 호스팅을 이용하여 사이트를 구축했다. 제한 트래픽은 100MB. 조그만 서비스에 100메가 쯤이야 무난할 것이라고 생각했다. 그런데 3월 4일 트래픽이 터졌다. 그것도 한 사람이 서버에 접속했는데 트래픽 용량이 초과되어 더 이상 작업이 불가능하단다. 이게 말이 되나? 굉장히 당황스러웠다.

![pic2](/assets/images/post/002/112_02.png)

고심하다 아마존웹서비스로 갈아타야겠다는 결심을 했다. 학생이라 매년 AWS 크레딧을 $100나 받고 있었다. 이전에 알아보니 애플 아이클라우드도 아마존웹서비스로 돌아가는 거라고 했다. 그럼 AWS는 내가 서버에다 뭘 해도 트래픽이 터지지 않겠지 싶었다. 바로 카페에서 자리를 잡아 AWS에 서버를 파기 시작했다. EC2, S3, RDS를 이용했다. 결과적으로 12시에 앉아서 9시에야 노트북을 끌 수 있었다. 꼼짝 않고 9시간동안 삽질을 했다. 서버를 열고 mySQL에 저장했던 데이터들을 이전할 수 있었지만 아마존웹서비스 무식자가 하루 만에 제대로 된 서버를 구축하여 로컬과 연결시킨다는 건 매우 고된 일이었다. 그리고 다음날, 닷홈 서버가 복구되어 닷홈을 계속 사용했다. 나 뭐한거지 그럼.

![pic3](/assets/images/post/002/112_03.png)
![pic4](/assets/images/post/002/112_04.png)
![pic5](/assets/images/post/002/112_05.png)
![pic6](/assets/images/post/002/112_06.png)
![pic7](/assets/images/post/002/112_07.png)

---

<br>

### warning

한글 인코딩 문제, 검색방식 문제로 워닝을 자주 봤다. utf-8로 저장하고 불러왔는데도 자꾸 euc-kr로 알아들었다. utf-8 쓰라고 제발!!! 개발자 친구의 도움으로 해결할 수 있었지만 순탄치 않았다.

![pic9](/assets/images/post/002/112_09.png)
![pic10](/assets/images/post/002/112_10.png)
![pic11](/assets/images/post/002/112_11.png)

---

<br>

### 그 후

숱한 오류를 거쳐 정보를 웹에 표현하는 게 가능해졌다. 그 과정 속에서 일정 딜레이는 이제 당연하듯 받아들이게 됐고 프로젝트를 엎지만 말자는 게 목표가 됐다. 네이버는 참 대단한 회사였다. 지마켓도 내가 뭐라 그랬지만 아주 훌륭한 플랫폼이었다. 이 세상에 존재하는 모든 웹/모바일 서비스는 절대 파워포인트 만지듯 뚝딱뚝딱 만들어지는 게 아니었다. 고객이었던 나는 '이 기능 구리다' 혹은 '이거 왜 이따구로 만들었니' 했지만 공급자 입장이 되어 보니 '나도 이거 구린 거 아는데 지금은 이게 최선이다', '내 눈엔 맘에 드는데 왜 그래'라고 하고 있었다.

이제 (아마도) css 위주로 작업하면 된다. 중간에 또 문제를 발견해 갈아 엎는 사태가 발생할 수도 있겠지만, 다시 하지 뭐. 트래픽이 터지지 않게, 데이터가 날라가지 않게, 인코딩 방식이 겹치지 않게, 코드 오류가 없게, 알고리즘에 하자가 없게 진행하면 되는 거다. 몇 주 사이 탈모가 생길 것 같다.

![pic12](/assets/images/post/002/112_12.png)
<figcaption class="caption">필요한 기능은 다 완성, css 적용하기 전</figcaption>

---
